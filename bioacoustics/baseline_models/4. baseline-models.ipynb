{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d550421a",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "A baseline model can be defined as a simple model that provides reasonable results on a task and requires not much time and expertise to develop. Baseline models helps put the more complex models into context in terms of accuracy. The results obtained from a baseline model should guide us in making the choice of complex model to use. In this classification task, we will use `Linear Support Vector Classifier`, `Support Vector Classifier`, `Multilayer Perceptron Classifier` and `Random Forests Classifiers` as our baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77327a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a0d7d",
   "metadata": {},
   "source": [
    "## Generate and save various parameters needed for the baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b762c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./parameters'):\n",
    "    os.makedirs('./parameters')\n",
    "\n",
    "np.save('./parameters/svm.npy', np.linspace(10e-2,10e2,200)) #C parameters for svm.LinearSVC and svm.SVC\n",
    "np.save('./parameters/svm-rbf.npy', np.linspace(10e-2,10e2,200)) #C parameters for svm.SVC\n",
    "np.save('./parameters/mlp.npy', np.arange(100,510, 10)) #hidden_layer_sizes for mlp classifier\n",
    "np.save('./parameters/rf.npy', np.arange(200, 2000, 200)) #n_estimators for Random Forest Classifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014e907",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Acoustic classification of bird species is often a challenging task due to some of the following reasons:\n",
    "<ol>\n",
    "<li>Background noise</li>\n",
    "<li>Variable length of sound recordings</li>\n",
    "<li>Few number of annotated recordings per species</li>\n",
    "</ol>\n",
    "\n",
    "For this task, we used various data preprocessing and data augmentation methods to tackle these problems. For background noise, we began with separating audio files into signal part (sections with a bird call/song) and noise part (parts with no bird call/song and that contain background noise). For the variable length of sound recordings, we split the audio into fixed size chunks to be fed to the models. Audio files that were less than the required fixed length were padded with noise to make them to be of at least the required fixed size. This in turn helped in increasing the number of inputs into the classifiers. From a single file, several chunks were obtained hence increasing the number of inputs to the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb880ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_mean_std(file_list):\n",
    "    \"\"\"Compute the mean and standard deviation of all the features\n",
    "    in file_list for use in normalisation\n",
    "    Args:\n",
    "        file_list: list of complete path to file to get features from\n",
    "    Returns:\n",
    "        mean: mean of all channels\n",
    "        std: standard deviation of all channels\n",
    "    \"\"\"\n",
    "    all_feature = np.array([])\n",
    "\n",
    "    for filename in file_list:\n",
    "\n",
    "        curr_feature = np.load(filename)\n",
    "        curr_feature = np.log(curr_feature + 1e-8)\n",
    "\n",
    "\n",
    "        if all_feature.size:\n",
    "            all_feature = np.vstack((all_feature, curr_feature.T))\n",
    "        else:\n",
    "            all_feature = curr_feature.T\n",
    "\n",
    "\n",
    "    return np.mean(all_feature, axis=0), np.std(all_feature, axis=0)\n",
    "\n",
    "\n",
    "def all_summary_features(file_list, annotation_dict, fmean, fstd, num_frame=10):\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in file_list:\n",
    "\n",
    "\n",
    "\n",
    "        feature = np.load(filename)\n",
    "        feature = np.log(feature + 1e-8)\n",
    "\n",
    "        feature = ((feature.T - fmean) /\n",
    "                       (fstd + 1e-8)).T\n",
    "\n",
    "        if feature.shape[1] > 2 * num_frame + 1:\n",
    "\n",
    "            for indx in range(num_frame, feature.shape[1] - num_frame - 1, num_frame):\n",
    "\n",
    "                current_feature = feature[:, indx - num_frame: indx + num_frame + 1]\n",
    "\n",
    "\n",
    "                features.append(np.concatenate((np.mean(current_feature, axis=1),\n",
    "                                                np.std(current_feature, axis=1))))\n",
    "\n",
    "\n",
    "                labels.append(annotation_dict[filename])\n",
    "\n",
    "    return np.array(features), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2518419",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d92aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(mels_dir, annotation_csv, model=None, param_dir=None):\n",
    "    \n",
    "    \n",
    "    # Get parameters from configuration file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('parameters.ini')\n",
    "\n",
    "    win_len_ms = int(config['audio']['win_len_ms'])\n",
    "    overlap = float(config['audio']['overlap'])\n",
    "    sampling_rate = int(config['audio']['sampling_rate'])\n",
    "    duration = float(config['neural-net']['input_duration_s'])\n",
    "    rnd_seed = int(config['neural-net']['seed'])\n",
    "\n",
    "\n",
    "    # Derive audio processing values\n",
    "    win_len = int((win_len_ms * sampling_rate) / 1000)\n",
    "    hop_len = int(win_len * (1 - overlap))\n",
    "    \n",
    "    np.random.seed(rnd_seed)\n",
    "    random.seed(rnd_seed)\n",
    "    tf.random.set_seed(rnd_seed)\n",
    "    labels = []\n",
    "    df_annotation = pd.read_csv(annotation_csv)\n",
    "    species = list(set(df_annotation['label']))\n",
    "    species.sort()\n",
    "    species_dict = dict(zip(species, range(len(species))))\n",
    "    for file_species in list(df_annotation['label']):\n",
    "        labels.append(species_dict[file_species])\n",
    "    \n",
    "    filelist = list(df_annotation.name)\n",
    "    for indx, filename in enumerate(filelist):\n",
    "        filelist[indx] = os.path.join(mels_dir, filename)\n",
    "    \n",
    "    annotation_dict = dict(zip(filelist, labels))\n",
    "    \n",
    "    training_files, validation_files = train_test_split(filelist, test_size=.3)\n",
    "    feature_mean, feature_std = compute_feature_mean_std(training_files[:])\n",
    "    num_frame = int((0.5 * duration * sampling_rate) / hop_len)\n",
    "    \n",
    "    X_train, y_train = all_summary_features(training_files,\n",
    "                                               annotation_dict,\n",
    "                                               feature_mean,\n",
    "                                               feature_std,\n",
    "                                               num_frame)\n",
    "    X_val, y_val = all_summary_features(validation_files,\n",
    "                                           annotation_dict,\n",
    "                                           feature_mean,\n",
    "                                           feature_std,\n",
    "                                           num_frame)\n",
    "    \n",
    "    \n",
    "    param_file = os.path.join(param_dir, model + '.npy')\n",
    "    parameters = np.load(param_file, allow_pickle=True)\n",
    "    \n",
    "    if model == 'svm':\n",
    "        clf = LinearSVC(random_state=0, tol=1e-5, multi_class='crammer_singer', max_iter=10000)\n",
    "    elif model == 'svm-rbf':\n",
    "        clf = SVC(gamma='auto')\n",
    "    elif model == 'mlp':\n",
    "        clf = MLPClassifier(random_state=1, max_iter=10000)\n",
    "    elif model == 'rf':\n",
    "        clf = RandomForestClassifier(max_depth=None, random_state=0)\n",
    "        \n",
    "        \n",
    "    val_accuracy = []\n",
    "    mean_f1_score = []\n",
    "    \n",
    "    params = tqdm(parameters)\n",
    "    for param in params:\n",
    "        if model == 'svm':\n",
    "            params.set_description(\"C = %s\" % param)\n",
    "            clf.set_params(C=param)\n",
    "            \n",
    "        if model == 'svm-rbf':\n",
    "            params.set_description(\"C = %s\" % param)\n",
    "            clf.set_params(C=param)\n",
    "\n",
    "        if model == 'mlp':\n",
    "            params.set_description(\"hidden_layer_sizes = %s\" % param)\n",
    "            clf.set_params(hidden_layer_sizes=param)\n",
    "\n",
    "        if model == 'rf':\n",
    "            params.set_description(\"n_estimators=param = %s\" % param)\n",
    "            clf.set_params(n_estimators=param)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_accuracy.append(sum(clf.predict(X_val) == y_val) / X_val.shape[0])\n",
    "        mean_f1_score.append(np.mean(f1_score(y_val, clf.predict(X_val), average=None)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    best_param = parameters[np.argmax(mean_f1_score)]\n",
    "\n",
    "    if 'svm' in model:\n",
    "        clf.set_params(C=best_param)\n",
    "\n",
    "    if model == 'mlp':\n",
    "        clf.set_params(hidden_layer_sizes=best_param)\n",
    "\n",
    "    if model == 'rf':\n",
    "        clf.set_params(n_estimators=best_param)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    species_f1_score = f1_score(y_val, clf.predict(X_val), average=None)\n",
    "    species_precision_score = precision_score(y_val, clf.predict(X_val), average=None)\n",
    "    species_recall_score = recall_score(y_val, clf.predict(X_val), average=None)\n",
    "    df_species_metrics = pd.DataFrame(list(zip(species,\n",
    "                                               species_precision_score,\n",
    "                                               species_recall_score,\n",
    "                                               species_f1_score)),\n",
    "                                      columns=['Species', 'Precision', 'Recall', 'F1 Score'])\n",
    "    df_species_metrics = df_species_metrics.sort_values(['F1 Score'], ascending=False)\n",
    "    df_species_metrics = df_species_metrics.reset_index(drop=True)\n",
    "\n",
    "    print('Mean F1 score:', np.mean(species_f1_score))\n",
    "    print(df_species_metrics.round(2))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261ee54",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ca620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main('./melspectrograms',\n",
    "         'labels.csv',\n",
    "         model='rf',\n",
    "         param_dir='parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c01bf0",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586af532",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main('./melspectrograms',\n",
    "         'labels.csv',\n",
    "         model='mlp',\n",
    "         param_dir='parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5a3ed",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20627750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main('./melspectrograms',\n",
    "         'labels.csv',\n",
    "         model='svm',\n",
    "         param_dir='parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52c329",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main('./melspectrograms',\n",
    "         'labels.csv',\n",
    "         model='svm-rbf',\n",
    "         param_dir='parameters')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
