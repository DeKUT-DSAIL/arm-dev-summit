{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d550421a",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "A baseline model can be defined as a simple model that provides reasonable results on a task and requires not much time and expertise to develop. Baseline models helps put the more complex models into context in terms of accuracy. The results obtained from a baseline model should guide us in making the choice of complex model to use. In this classification task, we will use `Linear Support Vector Classifier`, `Support Vector Classifier`, `Multilayer Perceptron Classifier` and `Random Forests Classifiers` as our baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77327a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a0d7d",
   "metadata": {},
   "source": [
    "## Generate and save various parameters needed for the baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b762c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir='./parameters'\n",
    "if not os.path.exists(param_dir):\n",
    "    os.makedirs(param_dir)\n",
    "\n",
    "np.save('./parameters/svm.npy', np.linspace(10e-2,10e2,200)) #C parameters for svm.LinearSVC and svm.SVC\n",
    "np.save('./parameters/svm-rbf.npy', np.linspace(10e-2,10e2,200)) #C parameters for svm.SVC\n",
    "np.save('./parameters/mlp.npy', np.arange(100,510, 10)) #hidden_layer_sizes for mlp classifier\n",
    "np.save('./parameters/rf.npy', np.arange(200, 2000, 200)) #n_estimators for Random Forest Classifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters from configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('parameters.ini')\n",
    "\n",
    "win_len_ms = int(config['audio']['win_len_ms'])\n",
    "overlap = float(config['audio']['overlap'])\n",
    "sampling_rate = int(config['audio']['sampling_rate'])\n",
    "duration = float(config['neural-net']['input_duration_s'])\n",
    "rnd_seed = int(config['neural-net']['seed'])\n",
    "\n",
    "\n",
    "# Derive audio processing values\n",
    "win_len = int((win_len_ms * sampling_rate) / 1000)\n",
    "hop_len = int(win_len * (1 - overlap))\n",
    "num_frame = int((0.5 * duration * sampling_rate) / hop_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b62978",
   "metadata": {},
   "source": [
    "### Generate a list of files and an annotation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e32d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models'\n",
    "annotation_csv = 'labels.csv'\n",
    "mels_dir = './melspectrograms'\n",
    "\n",
    "\n",
    "labels = []\n",
    "df_annotation = pd.read_csv(annotation_csv)\n",
    "\n",
    "species = list(set(df_annotation['label']))\n",
    "species.sort()\n",
    "species_dict = dict(zip(species, range(len(species))))\n",
    "species_dict\n",
    "for file_species in list(df_annotation['label']):\n",
    "    labels.append(species_dict[file_species])   \n",
    "filelist = list(df_annotation.name)\n",
    "for indx, filename in enumerate(filelist):\n",
    "    filelist[indx] = os.path.join(mels_dir, filename)\n",
    "    \n",
    "annotation_dict = dict(zip(filelist, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014e907",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Acoustic classification of bird species is often a challenging task due to some of the following reasons:\n",
    "<ol>\n",
    "<li>Background noise</li>\n",
    "<li>Variable length of sound recordings</li>\n",
    "<li>Few number of annotated recordings per species</li>\n",
    "</ol>\n",
    "\n",
    "For this task, we used various data preprocessing and data augmentation methods to tackle these problems. For background noise, we began with separating audio files into signal part (sections with a bird call/song) and noise part (parts with no bird call/song and that contain background noise). For the variable length of sound recordings, we split the audio into fixed size chunks to be fed to the models. Audio files that were less than the required fixed length were padded with noise to make them to be of at least the required fixed size. This in turn helped in increasing the number of inputs into the classifiers. From a single file, several chunks were obtained hence increasing the number of inputs to the classifiers. \n",
    "\n",
    "\n",
    "### Extracting mean and standard deviation of all channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_mean_std(file_list):\n",
    "    \"\"\"Compute the mean and standard deviation of all the features\n",
    "    in file_list for use in normalisation\n",
    "    Args:\n",
    "        file_list: list of complete path to file to get features from\n",
    "    Returns:\n",
    "        mean: mean of all channels\n",
    "        std: standard deviation of all channels\n",
    "    \"\"\"\n",
    "    all_feature = np.array([])\n",
    "\n",
    "    for filename in filelist:\n",
    "\n",
    "        curr_feature = np.load(filename)\n",
    "        curr_feature = np.log(curr_feature + 1e-8)\n",
    "\n",
    "\n",
    "        if all_feature.size:\n",
    "            all_feature = np.vstack((all_feature, curr_feature.T))\n",
    "        else:\n",
    "            all_feature = curr_feature.T\n",
    "\n",
    "\n",
    "    fmean = np.mean(all_feature, axis=0)\n",
    "    fstd = np.std(all_feature, axis=0)\n",
    "    \n",
    "    np.save('fmean.npy', fmean)\n",
    "    np.save('fstd.npy', fstd)\n",
    "    \n",
    "    return fmean, fstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa973",
   "metadata": {},
   "source": [
    "## Features standardization and visualization\n",
    "\n",
    "Pixels of an image need to be scaled before they are fed to a classification model. In this notebook, image standardization was used since it had the best performance. The standardization was done per dataset contrary to samplewise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmean, fstd = compute_feature_mean_std(filelist)\n",
    "\n",
    "bird = input('Enter the first name of a bird you downloaded: ')\n",
    "\n",
    "mels = [file for file in filelist if bird in file]\n",
    "mels\n",
    "\n",
    "indx = random.randint(0, len(mels) - 1)\n",
    "\n",
    "file = mels[indx]\n",
    "\n",
    "feature = np.load(file)\n",
    "\n",
    "feature_db = librosa.power_to_db(feature, ref=np.max)\n",
    "\n",
    "\n",
    "\n",
    "norm_feature = ((feature.T - fmean) / (fstd + 1e-8)).T\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(feature, ref=np.max),\n",
    "                         sr=sampling_rate,\n",
    "                         hop_length=hop_len,\n",
    "                         y_axis='mel', \n",
    "                         x_axis='time')\n",
    "\n",
    "plt.figure()\n",
    "librosa.display.specshow(librosa.amplitude_to_db(norm_feature, ref=np.max),\n",
    "                         sr=sampling_rate,\n",
    "                         hop_length=hop_len,\n",
    "                         y_axis='mel', \n",
    "                         x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42fc2a",
   "metadata": {},
   "source": [
    "### Features extraction and visualization\n",
    "We will split a normalized melspectrogram into chunks and extract the mean and standard deviation of the channels of a chunk and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if norm_feature.shape[1] > 2 * num_frame + 1:\n",
    "    count = 1\n",
    "\n",
    "    for indx in range(num_frame, norm_feature.shape[1] - num_frame - 1, num_frame):\n",
    "        \n",
    "        file_features = []\n",
    "\n",
    "        current_feature = norm_feature[:, indx - num_frame: indx + num_frame + 1]\n",
    "    \n",
    "        plt.figure()\n",
    "        plt.plot(np.mean(current_feature, axis=1), label='chunk' + str(count) + ' mean')\n",
    "        plt.plot(np.std(current_feature, axis=1),  label='chunk' + str(count) + ' std_deviation')\n",
    "        plt.xlabel('Channels')\n",
    "        plt.legend()\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635931e8",
   "metadata": {},
   "source": [
    "### Data normalization and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb880ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_summary_features(feature, filename, annotation_dict, fmean, fstd, num_frame):\n",
    "    \"\"\"Performs features standardization and data augmentation. \n",
    "    Args:feature- melspectrogram\n",
    "         filename- file name of the melspectrogram\n",
    "         annotation_dict- a dictionary containing file names and labels\n",
    "         fmean- mean of the entire melspectrograms dataset channels\n",
    "         fstd- standard deviation of the entire melspectrograms dataset channels\n",
    "         num_frame- number of frames.\n",
    "    \"\"\"\n",
    "\n",
    "    file_features = []\n",
    "    file_labels = []\n",
    "    \n",
    "    feature = ((feature.T - fmean) /\n",
    "                   (fstd + 1e-8)).T\n",
    "\n",
    "    if feature.shape[1] > 2 * num_frame + 1:\n",
    "\n",
    "        for indx in range(num_frame, feature.shape[1] - num_frame - 1, num_frame):\n",
    "\n",
    "            current_feature = feature[:, indx - num_frame: indx + num_frame + 1]\n",
    "\n",
    "\n",
    "            file_features.append(np.concatenate((np.mean(current_feature, axis=1),\n",
    "                                            np.std(current_feature, axis=1))))\n",
    "\n",
    "\n",
    "            file_labels.append(annotation_dict[filename])\n",
    "\n",
    "    return np.array(file_features), np.array(file_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d38259",
   "metadata": {},
   "source": [
    "### Train-validation-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af271d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split():\n",
    "    \"\"\"Returns training and validation files and labels\"\"\"\n",
    "    \n",
    "    np.random.seed(rnd_seed)\n",
    "    random.seed(rnd_seed)\n",
    "    tf.random.set_seed(rnd_seed)\n",
    "    \n",
    "    all_features = np.array([])\n",
    "    all_labels = np.array([])\n",
    "    \n",
    "    feature_mean, feature_std = compute_feature_mean_std(filelist)\n",
    "    \n",
    "    for filename in filelist:\n",
    "        file_labels = []\n",
    "\n",
    "        feature = np.load(filename)\n",
    "        feature = np.log(feature + 1e-8)\n",
    "        file_features, file_labels = all_summary_features(feature,\n",
    "                                                            filename,\n",
    "                                                            annotation_dict,\n",
    "                                                            feature_mean,\n",
    "                                                            feature_std,\n",
    "                                                            num_frame)\n",
    "        \n",
    "        if all_features.size:\n",
    "            all_features = np.vstack((all_features, file_features))\n",
    "        else:\n",
    "            all_features = file_features\n",
    "        all_labels = np.concatenate((all_labels, file_labels))\n",
    "        \n",
    "    X_train, X_val, y_train, y_val = train_test_split(all_features, all_labels, test_size=0.3)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2518419",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d92aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    param_file = os.path.join(param_dir, model + '.npy')\n",
    "    parameters = np.load(param_file, allow_pickle=True)\n",
    "    \n",
    "    if model == 'svm':\n",
    "        clf = LinearSVC(random_state=0, tol=1e-5, multi_class='crammer_singer', max_iter=10000)\n",
    "    elif model == 'svm-rbf':\n",
    "        clf = SVC(gamma='auto')\n",
    "    elif model == 'mlp':\n",
    "        clf = MLPClassifier(random_state=1, max_iter=10000)\n",
    "    elif model == 'rf':\n",
    "        clf = RandomForestClassifier(max_depth=None, random_state=0)\n",
    "        \n",
    "        \n",
    "    val_accuracy = []\n",
    "    mean_f1_score = []\n",
    "    \n",
    "    params = tqdm(parameters)\n",
    "    for param in params:\n",
    "        if model == 'svm':\n",
    "            params.set_description(\"C = %s\" % param)\n",
    "            clf.set_params(C=param)\n",
    "            \n",
    "        if model == 'svm-rbf':\n",
    "            params.set_description(\"C = %s\" % param)\n",
    "            clf.set_params(C=param)\n",
    "\n",
    "        if model == 'mlp':\n",
    "            params.set_description(\"hidden_layer_sizes = %s\" % param)\n",
    "            clf.set_params(hidden_layer_sizes=param)\n",
    "\n",
    "        if model == 'rf':\n",
    "            params.set_description(\"n_estimators=param = %s\" % param)\n",
    "            clf.set_params(n_estimators=param)\n",
    "            \n",
    "        X_train, X_val, y_train, y_val = train_val_split()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_accuracy.append(sum(clf.predict(X_val) == y_val) / X_val.shape[0])\n",
    "        mean_f1_score.append(np.mean(f1_score(y_val, clf.predict(X_val), average=None)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    best_param = parameters[np.argmax(mean_f1_score)]\n",
    "\n",
    "    if 'svm' in model:\n",
    "        clf.set_params(C=best_param)\n",
    "\n",
    "    if model == 'mlp':\n",
    "        clf.set_params(hidden_layer_sizes=best_param)\n",
    "\n",
    "    if model == 'rf':\n",
    "        clf.set_params(n_estimators=best_param)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "        \n",
    "    path = os.path.join(models_dir, model + '.pickle') #path to save models\n",
    "    pickle.dump(clf, open(path, 'wb'))\n",
    "    \n",
    "    \n",
    "    species_f1_score = f1_score(y_val, clf.predict(X_val), average=None)\n",
    "    species_precision_score = precision_score(y_val, clf.predict(X_val), average=None)\n",
    "    species_recall_score = recall_score(y_val, clf.predict(X_val), average=None)\n",
    "    df_species_metrics = pd.DataFrame(list(zip(species,\n",
    "                                               species_precision_score,\n",
    "                                               species_recall_score,\n",
    "                                               species_f1_score)),\n",
    "                                      columns=['Species', 'Precision', 'Recall', 'F1 Score'])\n",
    "    df_species_metrics = df_species_metrics.sort_values(['F1 Score'], ascending=False)\n",
    "    df_species_metrics = df_species_metrics.reset_index(drop=True)\n",
    "\n",
    "    print('Mean F1 score:', np.mean(species_f1_score))\n",
    "    print(df_species_metrics.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261ee54",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ca620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='rf'\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c01bf0",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586af532",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='mlp'\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5a3ed",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20627750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model='svm-rbf'\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52c329",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='svm'\n",
    "if __name__ == '__main__':\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64c559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
